## Papers
This repository contains list of the high impact that I have read, currently reading and intend to read. 

## 0. Transformers
- #### Done 
	- [Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention (Aug 2020)](https://arxiv.org/abs/2006.16236)
	- [Big Bird: Transformers for Longer Sequences (July 2020)](https://arxiv.org/abs/2007.14062)
	- [Multi-Head Attention: Collaborate Instead of Concatenate (June 2020)](https://arxiv.org/pdf/2006.16362.pdf)
	- [Dialogue Transformers (May 2020)](https://arxiv.org/abs/1910.00486)
	- [Efficient Attention: Attention with Linear Complexities (Jan 2020)](https://arxiv.org/pdf/1812.01243.pdf)
	- [Attention Is All You Need (June 2017](https://arxiv.org/pdf/1706.03762.pdf))
	- [NEURAL MACHINE TRANSLATION BY JOINTLY LEARNING TO ALIGN AND TRANSLATE (May 2016)](https://arxiv.org/pdf/1409.0473.pdf)
	
	
- #### To Do
	- [PAY ATTENTION WHEN REQUIRED (Sep 2019)](https://arxiv.org/abs/2009.04534)
	- [PAY ATTENTION WHEN REQUIRED (Sep 2019)](https://arxiv.org/abs/2009.04534)
	- 
	- [Are Sixteen Heads Really Better than One? (May 2019)](https://arxiv.org/pdf/1905.10650.pdf)


## 1. Domain Adaptation
- #### Done
    - [SENTIX: A Sentiment-Aware Pre-Trained Model forCross-Domain Sentiment Analysis (Nov 2020)](https://www.aclweb.org/anthology/2020.coling-main.49.pdf)
	- [Transformer Based Multi-Source Domain Adaptation (Nov 2020)](https://www.aclweb.org/anthology/2020.emnlp-main.639.pdf)
	- [Multi-Source Domain Adaptation with Mixture of Experts (Nov 2018)](https://www.aclweb.org/anthology/D18-1498.pdf)
	- [Neural Unsupervised Domain Adaptation in NLP—A Survey (Oct 2020)](https://arxiv.org/pdf/2006.00632.pdf)
	- [Domain Divergences: a Survey and Empirical Analysis (Oct 2020)](https://arxiv.org/abs/2010.12198)
	- [Don’t Stop Pretraining: Adapt Language Models to Domains and Tasks (July 2020)](https://www.aclweb.org/anthology/2020.acl-main.740.pdf)
	- [TX-Ray: Quantifying and Explaining Model-Knowledge Transfer in (Un-)Supervised NLP (Dec 2019)](https://arxiv.org/abs/1912.00982)
	- [To Annotate or Not? Predicting Performance Drop under Domain Shift (Nov 2019)](https://www.aclweb.org/anthology/D19-1222.pdf)
	- [What does BERT learn about the structure of language? (Aug 2019)](https://www.aclweb.org/anthology/P19-1356.pdf)
	- [Domain-Adversarial Training of Neural Networks (May 2015)](https://arxiv.org/abs/1505.07818)
	- [Learning Transferable Features with Deep Adaptation Networks (May 2015)](https://arxiv.org/pdf/1502.02791.pdf)

- #### To Do
    - [Evaluating Lottery Tickets Under Distributional Shifts (Nov 2019)](https://arxiv.org/abs/1910.12708)
	- [Investigating Transferability in Pretrained Language Models (Nov 2020](https://arxiv.org/pdf/2004.14975.pdf))
	- [KinGDOM: Knowledge-Guided DOMain adaptation for sentiment analysis (May 2020)](https://arxiv.org/pdf/2005.00791.pdf)
	- [Pretrained Transformers Improve Out-of-Distribution Robustness (April 2020)](https://arxiv.org/abs/2004.06100)
	- [What Happens To BERT Embeddings During Fine-tuning? (April 2020)](https://arxiv.org/pdf/2004.14448.pdf)
	- [CyCADA: Cycle-Consistent Adversarial Domain Adaptation (Nov 2017)](https://proceedings.mlr.press/v80/hoffman18a.html)
	- [How Transferable are Neural Networks in NLP Applications? (Mar 2019)](https://arxiv.org/abs/1603.06111)
	

## 2. Pretraining, Transfer Learning, Representation Learning
- #### Done 
	- [Distilling Knowledge Learned in BERT for Text Generation (Nov 2019)](https://arxiv.org/abs/1911.03829)
	- [DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter (Oct 2019)](https://arxiv.org/abs/1910.01108)
	- [Towards Language Agnostic Universal Representations (Sep 2019)](https://arxiv.org/pdf/1809.08510.pdf)
	- [What does BERT learn about the structure of language? (Aug 2019)](https://www.aclweb.org/anthology/P19-1356.pdf)
	- [RoBERTa: A Robustly Optimized BERT Pretraining Approach (July 2019)](https://arxiv.org/abs/1907.11692)
	- [XLNet: Generalized Autoregressive Pretraining for Language Understanding (June 2019)](https://arxiv.org/abs/1906.08237)
	- [Analyzing Multi-Head Self-Attention: Specialized Heads Do the Heavy Lifting, the Rest Can Be Pruned (May 2019)](https://arxiv.org/abs/1905.09418)
	- [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding (Oct 2018)](https://arxiv.org/abs/1810.04805)
	- [Improving Language Understandingby Generative Pre-Training](https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf)
	- 
- #### To Do
	- [Do Massively Pretrained Language Models Make Better Storytellers? (Sep 2019)](https://arxiv.org/pdf/1909.10705.pdf)
	- [GPT2: Language Models are Unsupervised Multitask Learners (2019)](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)

## 3. Conversation AI
### 3.1 Dialogue Act, Intent and Emotion 
- #### Done
	- [Towards Emotion-aided Multi-modal Dialogue Act Classification (July 2020)](https://www.aclweb.org/anthology/2020.acl-main.402/)
	-  [Contextual Dialogue Act Classification for Open-Domain Conversational Agents (May 2020)](https://arxiv.org/abs/2005.13804)
	-  [An Evaluation Dataset for Intent Classification and Out-of-Scope Prediction (Sep 2019)](https://www.aclweb.org/anthology/D19-1131.pdf)
	-  [Dialogue Act Classification with Context-Aware Self-Attention (May 2019)](https://www.aclweb.org/anthology/N19-1373/)
	-  [BERT for Joint Intent Classification and Slot Filling (Feb 2019)](https://arxiv.org/abs/1902.10909)
	-  [Context-Aware Self-Attention Networks (Feb 2019)](https://arxiv.org/abs/1902.05766)
	-  [A Context-based Approach for Dialogue Act Recognition using Simple Recurrent Neural Networks (May 2018)](https://www.aclweb.org/anthology/L18-1307/)
	-  [Utterance Intent Classification of a Spoken Dialogue System with Efficiently Untied Recursive Autoencoders (Aug 2017)](https://www.aclweb.org/anthology/W17-5508/)
- #### To Do
	- [Modeling Dialogue in Conversational Cognitive Health Screening Interviews (May 2020)](https://www.aclweb.org/anthology/2020.lrec-1.147/)

### 3.2 Dialogue Generation
- #### Done
	- [Neural Generation Meets Real People: Towards Emotionally Engaging Mixed-Initiative Conversations (Aug 2020)](https://arxiv.org/abs/2008.12348)
	- [DIALOGPT : Large-Scale Generative Pre-trainingfor Conversational Response Generation (May 2020)](https://arxiv.org/pdf/1911.00536.pdf)
	- [A Simple Language Model for Task-Oriented Dialogue (May 2020)](https://arxiv.org/abs/2005.00796)
	- [CAiRE: An Empathetic Neural Chatbot (April 2020)](https://arxiv.org/abs/1907.12108)
	- [Neural Assistant: Joint Action Prediction, Response Generation, and Latent Knowledge Reasoning (Oct 2019)](https://arxiv.org/abs/1910.14613)
	- [Do Neural Dialog Systems Use the Conversation History Effectively? An Empirical Study (July 2019)](https://arxiv.org/abs/1906.01603)
	- [What makes a good conversation? How controllable attributes affect human judgments (April 2019)](https://arxiv.org/abs/1902.08654?)
	- [Towards Empathetic Open-domain Conversation Models: a New Benchmark and Dataset (Nov 2018)](https://arxiv.org/abs/1811.00207)
	- [Toward Continual Learning for Conversational Agents (Jan 2018)](https://arxiv.org/abs/1712.09943)
	- [Delivering Cognitive Behavior Therapy to Young Adults With Symptoms of Depression and Anxiety Using a Fully Automated Conversational Agent (Woebot): A Randomized Controlled Trial (May 2017)](https://pubmed.ncbi.nlm.nih.gov/28588005/)
	- [Building End-To-End Dialogue Systems Using Generative Hierarchical Neural Network Models (April 2016)](https://arxiv.org/abs/1507.04808)
	- [A Neural Conversational Model (Jul 2015](https://arxiv.org/abs/1506.05869))
- #### To Do
	- [PONE: A Novel Automatic Evaluation Metric for Open-Domain Generative Dialogue Systems (April 2020)](https://arxiv.org/abs/2004.02399)
	- [Delivering Cognitive Behavior Therapy to Young Adults With Symptoms of Depression and Anxiety Using a Fully Automated Conversational Agent (Woebot): A Randomized Controlled Trial (2017)](www.google.com)
	

## 4. Summarization, Translation Paraphrasing and Other Seq2Seq  
- #### Done
	- [Unsupervised Paraphrasing by Simulated Annealing (July 2020)](https://www.aclweb.org/anthology/2020.acl-main.28.pdf)
	- [Pretraining-Based Natural Language Generation for Text Summarization (Nov 2019)](https://www.aclweb.org/anthology/K19-1074.pdf)
	- [MASS: Masked Sequence to Sequence Pre-training for Language Generation (May 2019)](https://arxiv.org/abs/1905.02450)
	
- #### To Do
	- [Very Deep Transformers for Neural Machine Translation (Aug 2020)](https://arxiv.org/abs/2008.07772)


## 5. Miscellaneous
- #### Done
    - [When BERT Plays the Lottery, All Tickets Are Winning (Nov 2020)](https://arxiv.org/abs/2005.00561)
    - [Deconstructing Lottery Tickets: Zeros, Signs, and the Supermask (Dec 2019)](https://arxiv.org/abs/1905.01067)
    - [The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks (May 2019)](https://arxiv.org/abs/1803.03635)
	- [An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale (Oct 2020)](https://openreview.net/pdf?id=YicbFdNTTy)
	- [Are Transformers universal approximators of sequence-to-sequence functions? (Dec 2019)](https://arxiv.org/abs/1912.10077)

- #### To Do
	- [Beyond Accuracy: Behavioral Testing of NLP Models with CheckList (May 2020)](https://arxiv.org/abs/2005.04118)
	- [Are distributional representations ready for the real world? Evaluating word vectors for grounded perceptual meaning (May 2017)](https://arxiv.org/abs/1705.11168)
	- [Understanding Attention for Text Classification (July 2020)](https://www.aclweb.org/anthology/2020.acl-main.312/)